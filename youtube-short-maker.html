<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Auto-Cut Clip Maker — Free (Client-side)</title>
<style>
  body{font-family:system-ui,Arial;margin:14px;color:#111}
  h1{margin:0 0 6px}
  .row{display:flex;gap:12px;flex-wrap:wrap}
  .panel{border:1px solid #ddd;padding:12px;border-radius:8px;min-width:320px;flex:1}
  label{display:block;margin:8px 0 4px;font-weight:600}
  input[type=file]{padding:6px}
  button{padding:8px 12px;border-radius:8px;border:0;background:#0b76ff;color:#fff;cursor:pointer}
  button.secondary{background:#666}
  canvas{background:#000;display:block;margin:8px 0;max-width:100%}
  #log{white-space:pre-wrap;font-family:monospace;margin-top:8px;background:#f8f8f8;padding:8px;border-radius:6px}
  .small{font-size:13px;color:#555}
  .clips{display:flex;flex-direction:column;gap:8px;margin-top:8px}
  .clip-item{padding:8px;border:1px dashed #ccc;border-radius:6px;display:flex;gap:8px;align-items:center;justify-content:space-between}
  .meta{font-size:13px;color:#333}
</style>
</head>
<body>
  <h1>Auto-Cut Clip Maker — Free (Client-side)</h1>
  <div class="small">Upload a video → Automatic scene detection (frame-difference) → Preview detected clips → Export WebM clips (client-side).</div>

  <div class="row" style="margin-top:12px">
    <div class="panel">
      <label>1) Upload video</label>
      <input id="file" type="file" accept="video/*">
      <div id="video-info" class="small" style="margin-top:8px"></div>

      <label>2) Scene detection settings</label>
      <div class="small">Frame sample interval (ms)</div>
      <input id="sampleMs" type="number" value="250" min="50" step="50">
      <div class="small">Difference threshold (0-1) — lower = more sensitive</div>
      <input id="threshold" type="number" value="0.18" min="0.01" max="1" step="0.01">
      <div style="margin-top:8px;display:flex;gap:8px">
        <button id="scanBtn">Scan & Detect Scenes</button>
        <button id="autoTrimBtn" class="secondary">Auto Trim Short Clips</button>
      </div>

      <label style="margin-top:12px">3) Optional: captions to burn into clips</label>
      <textarea id="captions" placeholder="Optional text to burn into every clip (multiple lines allowed)" style="width:100%;min-height:60px"></textarea>

      <div style="margin-top:10px">
        <button id="exportAll" class="secondary">Export All Detected Clips</button>
      </div>
    </div>

    <div class="panel">
      <label>Preview</label>
      <canvas id="previewCanvas" width="720" height="405"></canvas>
      <div style="display:flex;gap:8px;align-items:center;margin-top:8px">
        <button id="playPreview">Play</button>
        <button id="pausePreview" class="secondary">Pause</button>
        <div id="currentTime" class="small" style="margin-left:8px"></div>
      </div>

      <label style="margin-top:12px">Detected clips</label>
      <div id="clips" class="clips"></div>

      <div id="log"></div>
    </div>
  </div>

  <video id="sourceVideo" playsinline style="display:none"></video>

<script>
/*
  Approach:
  - Sample frames from the video at regular intervals (sampleMs).
  - For each sampled frame, compute a small grayscale downsample and calculate
    average absolute difference vs previous frame.
  - If difference > threshold => mark a scene cut.
  - Build clip ranges between cuts (merge very short segments).
  - Allow preview and export each clip using canvas capture + original audio (captureStream).
*/

const fileInput = document.getElementById('file');
const video = document.getElementById('sourceVideo');
const info = document.getElementById('video-info');
const scanBtn = document.getElementById('scanBtn');
const sampleMsEl = document.getElementById('sampleMs');
const thresholdEl = document.getElementById('threshold');
const previewCanvas = document.getElementById('previewCanvas');
const ctx = previewCanvas.getContext('2d');
const clipsContainer = document.getElementById('clips');
const captionsInput = document.getElementById('captions');
const autoTrimBtn = document.getElementById('autoTrimBtn');
const exportAllBtn = document.getElementById('exportAll');
const playBtn = document.getElementById('playPreview');
const pauseBtn = document.getElementById('pausePreview');
const currentTimeEl = document.getElementById('currentTime');
const logEl = document.getElementById('log');

let fileURL = null;
let videoReady = false;
let detections = []; // array of cut times (seconds)
let clipRanges = []; // array of {start,end}
let sampling = false;

function log(...a){ logEl.textContent = a.join(' ') }

fileInput.addEventListener('change', (e)=>{
  if (fileURL) URL.revokeObjectURL(fileURL);
  const f = e.target.files[0];
  if (!f) return;
  fileURL = URL.createObjectURL(f);
  video.src = fileURL;
  video.load();
  videoReady = false;
  info.textContent = `Loaded: ${f.name} (${Math.round(f.size/1024)} KB)`;
  detections = []; clipRanges = []; renderClipsUI();
  log('Video loaded. Wait for metadata.');
});

video.addEventListener('loadedmetadata', ()=>{
  videoReady = true;
  previewCanvas.width = Math.min(960, video.videoWidth);
  previewCanvas.height = Math.round(previewCanvas.width * (9/16)); // aesthetic preview
  info.textContent += ` — ${video.videoWidth}x${video.videoHeight}, ${video.duration.toFixed(2)}s`;
  log('Metadata ready. You can scan now.');
});

// Utility: downsample a frame into small grayscale array
function getFrameGray(scaleW=64, scaleH=36){
  const w = scaleW, h = scaleH;
  const offscreen = document.createElement('canvas');
  offscreen.width = w; offscreen.height = h;
  const g = offscreen.getContext('2d');
  // draw current video frame scaled down
  try { g.drawImage(video, 0, 0, video.videoWidth, video.videoHeight, 0, 0, w, h); }
  catch(e){ return null; }
  const data = g.getImageData(0,0,w,h).data;
  const gray = new Float32Array(w*h);
  for (let i=0;i<w*h;i++){
    const r = data[i*4], gg = data[i*4+1], b = data[i*4+2];
    gray[i] = (0.299*r + 0.587*gg + 0.114*b) / 255;
  }
  return {w,h,gray};
}

function diffFrames(a, b){
  // average absolute difference
  if (!a || !b) return 1;
  const n = a.gray.length;
  let s = 0;
  for (let i=0;i<n;i++){
    s += Math.abs(a.gray[i] - b.gray[i]);
  }
  return s / n;
}

// Scene scan
scanBtn.addEventListener('click', async ()=>{
  if (!videoReady){ log('Load a video first'); return; }
  detections = []; clipRanges = []; renderClipsUI();
  const sampleMs = Number(sampleMsEl.value) || 250;
  const threshold = Number(thresholdEl.value) || 0.18;
  log(`Scanning: sample ${sampleMs}ms, threshold ${threshold}. This runs in your browser.`);
  sampling = true;

  // pause native playback
  video.pause();
  // sample across duration
  const dur = video.duration;
  const samples = Math.max(3, Math.floor((dur*1000) / sampleMs));
  let prevFrame = null;
  for (let i=0;i<=samples && sampling;i++){
    const t = Math.min(dur, (i*sampleMs)/1000);
    await seekTo(t);
    // small delay for frame to be ready
    await new Promise(r => setTimeout(r, 30));
    const frame = getFrameGray(64, 36);
    const d = diffFrames(frame, prevFrame);
    // log progress occasionally
    if (i % Math.max(1, Math.floor(samples/20)) === 0) {
      log(`Scanning ${Math.round((i/samples)*100)}% — time ${t.toFixed(2)}s — diff ${d.toFixed(3)}`);
    }
    if (prevFrame && d > threshold){
      // detected cut at time t
      detections.push(t);
    }
    prevFrame = frame;
  }

  sampling = false;
  log(`Scan complete — ${detections.length} cuts detected.`);
  buildClipRanges();
  renderClipsUI();
});

// helper: seek and wait
function seekTo(t){ return new Promise(res=>{
  const onSeek = ()=>{ video.removeEventListener('seeked', onSeek); res(); };
  video.addEventListener('seeked', onSeek);
  try { video.currentTime = Math.min(video.duration, Math.max(0, t)); }
  catch(e){ setTimeout(res, 120); }
  // safety timeout
  setTimeout(()=>{
    video.removeEventListener('seeked', onSeek);
    res();
  }, 400);
});}

// build ranges between cuts (start at 0)
function buildClipRanges(){
  const dur = video.duration;
  const pts = [0, ...detections.filter(t=>t>0 && t<dur), dur];
  const rawRanges = [];
  for (let i=0;i<pts.length-1;i++){
    rawRanges.push({start: pts[i], end: pts[i+1]});
  }
  // merge too-short clips (<0.8s) into neighbors
  const minLen = 0.8;
  const merged = [];
  for (let r of rawRanges){
    if (merged.length===0) merged.push(r);
    else {
      const last = merged[merged.length-1];
      if ((r.end - r.start) < minLen){
        last.end = r.end; // merge small piece to previous
      } else merged.push(r);
    }
  }
  // also filter very long ranges if you want (here we keep all)
  clipRanges = merged;
}

// display UI list of clips
function renderClipsUI(){
  clipsContainer.innerHTML = '';
  if (clipRanges.length===0){ clipsContainer.textContent = 'No clips yet. Run scan.'; return; }
  clipRanges.forEach((r, idx)=>{
    const el = document.createElement('div');
    el.className = 'clip-item';
    const left = document.createElement('div');
    left.className = 'meta';
    left.innerHTML = `Clip ${idx+1} — ${r.start.toFixed(2)}s → ${r.end.toFixed(2)}s (${(r.end-r.start).toFixed(2)}s)`;
    const right = document.createElement('div');
    const btnPreview = document.createElement('button');
    btnPreview.textContent = 'Preview';
    btnPreview.onclick = ()=> previewRange(r.start, r.end);
    const btnExport = document.createElement('button');
    btnExport.textContent = 'Export';
    btnExport.style.marginLeft = '8px';
    btnExport.onclick = ()=> exportRange(r.start, r.end, idx+1);
    right.appendChild(btnPreview); right.appendChild(btnExport);
    el.appendChild(left); el.appendChild(right);
    clipsContainer.appendChild(el);
  });
}

// Preview a range: play video and draw to canvas
let previewRAF = null;
function previewRange(s,e){
  if (!videoReady) return;
  video.currentTime = s;
  video.muted = true;
  video.play().catch(()=>{});
  const cw = previewCanvas.width, ch = previewCanvas.height;
  function loop(){
    const now = video.currentTime;
    if (now >= e || video.ended){ video.pause(); cancelAnimationFrame(previewRAF); previewRAF=null; return; }
    // draw center-fit
    try {
      const srcAR = video.videoWidth / video.videoHeight;
      const dstAR = cw / ch;
      let sx, sy, sw, sh;
      if (srcAR > dstAR){
        sh = video.videoHeight;
        sw = Math.round(sh * dstAR);
        sx = Math.round((video.videoWidth - sw) / 2); sy = 0;
      } else {
        sw = video.videoWidth; sh = Math.round(sw / dstAR);
        sx = 0; sy = Math.round((video.videoHeight - sh) / 2);
      }
      ctx.fillStyle = 'black';
      ctx.fillRect(0,0,cw,ch);
      ctx.drawImage(video, sx, sy, sw, sh, 0, 0, cw, ch);
      // captions
      const txt = captionsInput.value.trim();
      if (txt){
        ctx.font = `bold ${Math.floor(cw/20)}px system-ui`;
        ctx.textAlign = 'center'; ctx.fillStyle='white';
        const lines = txt.split('\n');
        const blockY = ch - 60 - (lines.length*26);
        ctx.fillStyle = 'rgba(0,0,0,0.45)'; ctx.fillRect(cw*0.05, blockY-10, cw*0.9, (lines.length*26)+20);
        ctx.fillStyle = 'white';
        for (let i=0;i<lines.length;i++){
          ctx.fillText(lines[i], cw/2, blockY + (i*26) + 20);
        }
      }
      currentTimeEl.textContent = now.toFixed(2)+'s';
    } catch(e){}
    previewRAF = requestAnimationFrame(loop);
  }
  if (previewRAF) cancelAnimationFrame(previewRAF);
  previewRAF = requestAnimationFrame(loop);
}

// export a single range: record canvas (visual) + original audio (from captureStream)
async function exportRange(s,e,idx){
  if (!videoReady){ log('Load video first'); return; }
  log(`Exporting clip ${idx} ${s.toFixed(2)}s → ${e.toFixed(2)}s`);
  // prepare canvas size (use 720 width)
  const cw = 720;
  const ch = Math.round(cw * 9/16);
  previewCanvas.width = cw; previewCanvas.height = ch;

  // unmute video so captureStream includes audio on some browsers
  video.muted = false;
  video.currentTime = s;
  await seekTo(s);
  video.play().catch(()=>{});

  // draw frames to canvas while recording
  let raf;
  function draw(){
    const now = video.currentTime;
    if (now >= e || video.ended){ cancelAnimationFrame(raf); return; }
    const srcAR = video.videoWidth / video.videoHeight;
    const dstAR = cw / ch;
    let sx, sy, sw, sh;
    if (srcAR > dstAR){
      sh = video.videoHeight; sw = Math.round(sh * dstAR); sx = Math.round((video.videoWidth - sw)/2); sy=0;
    } else {
      sw = video.videoWidth; sh = Math.round(sw / dstAR); sx=0; sy = Math.round((video.videoHeight - sh)/2);
    }
    ctx.fillStyle = 'black'; ctx.fillRect(0,0,cw,ch);
    try { ctx.drawImage(video, sx, sy, sw, sh, 0, 0, cw, ch); } catch(e){}
    // captions
    const txt = captionsInput.value.trim();
    if (txt){
      ctx.font = `bold ${Math.floor(cw/20)}px system-ui`; ctx.textAlign='center';
      const lines = txt.split('\n');
      const blockY = ch - 60 - (lines.length*26);
      ctx.fillStyle = 'rgba(0,0,0,0.45)'; ctx.fillRect(cw*0.05, blockY-10, cw*0.9, (lines.length*26)+20);
      ctx.fillStyle='white';
      for (let i=0;i<lines.length;i++) ctx.fillText(lines[i], cw/2, blockY + (i*26) + 20);
    }
    raf = requestAnimationFrame(draw);
  }
  raf = requestAnimationFrame(draw);

  // merge canvas video track + original audio tracks (from video.captureStream)
  const vStream = video.captureStream ? video.captureStream() : null;
  if (!vStream){ log('captureStream not supported in this browser. Use Chrome/Edge'); return; }
  const audioTracks = vStream.getAudioTracks();
  const canvasStream = previewCanvas.captureStream(30);
  const combined = new MediaStream();
  combined.addTrack(canvasStream.getVideoTracks()[0]);
  audioTracks.forEach(t=>combined.addTrack(t));

  // recorder
  const mime = MediaRecorder.isTypeSupported('video/webm;codecs=vp8,opus') ? 'video/webm;codecs=vp8,opus' : 'video/webm';
  const rec = new MediaRecorder(combined, {mimeType: mime});
  const chunks = [];
  rec.ondataavailable = e => { if (e.data && e.data.size) chunks.push(e.data); };
  rec.start(200);

  // wait until end time
  await new Promise(res=>{
    const onTime = ()=>{
      if (video.currentTime >= e || video.ended){
        video.removeEventListener('timeupdate', onTime); res();
      }
    };
    video.addEventListener('timeupdate', onTime);
  });

  // stop
  rec.stop();
  await new Promise(r=> setTimeout(r, 300));
  try { video.pause(); } catch(e){}
  if (raf) cancelAnimationFrame(raf);

  const blob = new Blob(chunks, {type: mime});
  const url = URL.createObjectURL(blob);
  // create download link
  const a = document.createElement('a');
  a.href = url; a.download = `clip_${idx}_${Math.floor(s)}_${Math.floor(e)}.webm`;
  a.textContent = `Download Clip ${idx} (${(e-s).toFixed(2)}s)`;
  a.style.display = 'inline-block';
  a.style.padding = '6px 8px';
  a.style.background = '#0b76ff';
  a.style.color = 'white';
  a.style.borderRadius = '6px';
  // attach to clip UI area
  const wrapper = document.createElement('div');
  wrapper.style.marginTop = '6px';
  wrapper.appendChild(a);
  clipsContainer.appendChild(wrapper);
  log(`Exported clip ${idx}`);
}

// auto trim button: split into short clips of max duration (e.g., 60s)
autoTrimBtn.addEventListener('click', ()=>{
  if (!videoReady){ log('Load video first'); return; }
  const maxLen = 60; // seconds
  const dur = video.duration;
  clipRanges = [];
  for (let s=0; s<dur; s+=maxLen){
    clipRanges.push({start: s, end: Math.min(dur, s+maxLen)});
  }
  renderClipsUI();
  log('Auto-trim into ' + clipRanges.length + ' clips of up to ' + maxLen + 's');
});

// Export all detected clips sequentially (user-friendly)
exportAllBtn.addEventListener('click', async ()=>{
  if (clipRanges.length===0){ log('No clips to export'); return; }
  for (let i=0;i<clipRanges.length;i++){
    await exportRange(clipRanges[i].start, clipRanges[i].end, i+1);
    // small pause so user sees progress
    await new Promise(r=>setTimeout(r, 400));
  }
  log('All exports attempted (download links appended).');
});

// simple play/pause controls for overall video (not range-specific)
playBtn.addEventListener('click', ()=>{ video.muted=true; video.play().catch(()=>{}); });
pauseBtn.addEventListener('click', ()=>{ video.pause(); });

</script>
</body>
</html>
